{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMRC.ipynb",
      "provenance": [],
      "mount_file_id": "1HRXlJXomUz2dflUY7OHxEuaY1X0D9kQb",
      "authorship_tag": "ABX9TyN16jonRti78UPbY/FVy0yd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shinkai125/Band/blob/master/CMRC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ltbu_BhEk1d",
        "outputId": "9ea087b3-fbfd-4147-9b7a-6233ceb3e26c"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Apr 25 13:10:04 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVlbauGfFFGU",
        "outputId": "2b7912fa-7853-4f97-d732-8b3f76fea52b"
      },
      "source": [
        "import os\n",
        "path = \"/content/drive/My Drive\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab Notebooks',\n",
              " '.ipynb_checkpoints',\n",
              " 'cmrc2018',\n",
              " 'chinese_roberta_wwm_ext_pytorch']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVMwrQbGIcCe",
        "outputId": "db7c3f59-195c-47b9-b50f-b67c40eaab12"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\r\u001b[K     |▏                               | 10kB 19.6MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 26.7MB/s eta 0:00:01\r\u001b[K     |▌                               | 30kB 24.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 19.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 51kB 15.3MB/s eta 0:00:01\r\u001b[K     |█                               | 61kB 13.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 71kB 13.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 81kB 12.6MB/s eta 0:00:01\r\u001b[K     |█▍                              | 92kB 12.5MB/s eta 0:00:01\r\u001b[K     |█▋                              | 102kB 12.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 112kB 12.5MB/s eta 0:00:01\r\u001b[K     |██                              | 122kB 12.5MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 143kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▍                             | 153kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▌                             | 163kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 174kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 184kB 12.5MB/s eta 0:00:01\r\u001b[K     |███                             | 194kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▏                            | 204kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▍                            | 215kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 225kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▋                            | 235kB 12.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 245kB 12.5MB/s eta 0:00:01\r\u001b[K     |████                            | 256kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 266kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 276kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 286kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 296kB 12.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 307kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 317kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 327kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 337kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 348kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 358kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 368kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 378kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 389kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 399kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 409kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 419kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 430kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 440kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 450kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 460kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 471kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 481kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 491kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 501kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 512kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 522kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 532kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 542kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 552kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 563kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 573kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 583kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 593kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 604kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 614kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 624kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 634kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 645kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 655kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 665kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 675kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 686kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 696kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 706kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 716kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 727kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 737kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 747kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 757kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 768kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 778kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 788kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 798kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 808kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 819kB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 829kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 839kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 849kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 860kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 870kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 880kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 890kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 901kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 911kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 921kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 931kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 942kB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 952kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 962kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 972kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 983kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 993kB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 1.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.2MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.3MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.4MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.5MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.6MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.7MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.8MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.9MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 2.0MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 12.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.1MB 12.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 57.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmY7YiSkI7nZ",
        "outputId": "c37813e0-0844-4554-efdc-ed3648380db8"
      },
      "source": [
        "!pip install tensorflow-gpu -U"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/cc/a27e73cf8b23f2ce4bdd2b7089a42a7819ce6dd7366dceba406ddc5daa9c/tensorflow_gpu-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3MB)\n",
            "\u001b[K     |████████████████████████████████| 394.3MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12)\n",
            "Requirement already satisfied, skipping upgrade: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (3.3.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.8.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow-gpu) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.2.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (4.7.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow-gpu) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow-gpu) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow-gpu) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow-gpu) (3.4.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qXPAQVlKb4z"
      },
      "source": [
        "import argparse\n",
        "import io\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "def _tokenize_chinese_chars(text):\n",
        "    \"\"\"\n",
        "    :param text: input text, unicode string\n",
        "    :return:\n",
        "        tokenized text, list\n",
        "    \"\"\"\n",
        "\n",
        "    def _is_chinese_char(cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #     https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n",
        "                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n",
        "                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n",
        "                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n",
        "                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
        "                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n",
        "                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
        "                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    output = []\n",
        "    buff = \"\"\n",
        "    for char in text:\n",
        "        cp = ord(char)\n",
        "        if _is_chinese_char(cp) or char == \"=\":\n",
        "            if buff != \"\":\n",
        "                output.append(buff)\n",
        "                buff = \"\"\n",
        "            output.append(char)\n",
        "        else:\n",
        "            buff += char\n",
        "\n",
        "    if buff != \"\":\n",
        "        output.append(buff)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def _normalize(in_str):\n",
        "    \"\"\"\n",
        "    normalize the input unicode string\n",
        "    \"\"\"\n",
        "    in_str = in_str.lower()\n",
        "    sp_char = [\n",
        "        u':', u'_', u'`', u'，', u'。', u'：', u'？', u'！', u'(', u')',\n",
        "        u'“', u'”', u'；', u'’', u'《', u'》', u'……', u'·', u'、', u',',\n",
        "        u'「', u'」', u'（', u'）', u'－', u'～', u'『', u'』', '|'\n",
        "    ]\n",
        "    out_segs = []\n",
        "    for char in in_str:\n",
        "        if char in sp_char:\n",
        "            continue\n",
        "        else:\n",
        "            out_segs.append(char)\n",
        "    return ''.join(out_segs)\n",
        "\n",
        "\n",
        "def find_lcs(s1, s2):\n",
        "    \"\"\"find the longest common subsequence between s1 ans s2\"\"\"\n",
        "    m = [[0 for i in range(len(s2) + 1)] for j in range(len(s1) + 1)]\n",
        "    max_len = 0\n",
        "    p = 0\n",
        "    for i in range(len(s1)):\n",
        "        for j in range(len(s2)):\n",
        "            if s1[i] == s2[j]:\n",
        "                m[i + 1][j + 1] = m[i][j] + 1\n",
        "                if m[i + 1][j + 1] > max_len:\n",
        "                    max_len = m[i + 1][j + 1]\n",
        "                    p = i + 1\n",
        "    return s1[p - max_len:p], max_len\n",
        "\n",
        "\n",
        "def evaluate(ref_ans, pred_ans, verbose=False):\n",
        "    \"\"\"\n",
        "    ref_ans: reference answers, dict\n",
        "    pred_ans: predicted answer, dict\n",
        "    return:\n",
        "        f1_score: averaged F1 score\n",
        "        em_score: averaged EM score\n",
        "        total_count: number of samples in the reference dataset\n",
        "        skip_count: number of samples skipped in the calculation due to unknown errors\n",
        "    \"\"\"\n",
        "    f1 = 0\n",
        "    em = 0\n",
        "    total_count = 0\n",
        "    skip_count = 0\n",
        "    for document in ref_ans:\n",
        "        para = document[1].strip()\n",
        "        total_count += 1\n",
        "        query_id = document[0]\n",
        "        query_text = document[2].strip()\n",
        "        answers = document[3]\n",
        "        try:\n",
        "            prediction = pred_ans[str(query_id)]\n",
        "        except:\n",
        "            skip_count += 1\n",
        "            if verbose:\n",
        "                print(\"para: {}\".format(para))\n",
        "                print(\"query: {}\".format(query_text))\n",
        "                print(\"ref: {}\".format('#'.join(answers)))\n",
        "                print(\"Skipped\")\n",
        "                print('----------------------------')\n",
        "            continue\n",
        "        _f1 = calc_f1_score(answers, prediction)\n",
        "        f1 += _f1\n",
        "        em += calc_em_score(answers, prediction)\n",
        "        if verbose:\n",
        "            print(\"para: {}\".format(para))\n",
        "            print(\"query: {}\".format(query_text))\n",
        "            print(\"ref: {}\".format('#'.join(answers)))\n",
        "            print(\"cand: {}\".format(prediction))\n",
        "            print(\"score: {}\".format(_f1))\n",
        "            print('----------------------------')\n",
        "\n",
        "    f1_score = 100.0 * f1 / total_count\n",
        "    em_score = 100.0 * em / total_count\n",
        "    return f1_score, em_score, total_count, skip_count\n",
        "\n",
        "\n",
        "def calc_f1_score(answers, prediction):\n",
        "    f1_scores = []\n",
        "    for ans in answers:\n",
        "        ans_segs = _tokenize_chinese_chars(_normalize(ans))\n",
        "        prediction_segs = _tokenize_chinese_chars(_normalize(prediction))\n",
        "        lcs, lcs_len = find_lcs(ans_segs, prediction_segs)\n",
        "        if lcs_len == 0:\n",
        "            f1_scores.append(0)\n",
        "            continue\n",
        "        prec = 1.0 * lcs_len / len(prediction_segs)\n",
        "        rec = 1.0 * lcs_len / len(ans_segs)\n",
        "        f1 = (2 * prec * rec) / (prec + rec)\n",
        "        f1_scores.append(f1)\n",
        "    return max(f1_scores)\n",
        "\n",
        "\n",
        "def calc_em_score(answers, prediction):\n",
        "    em = 0\n",
        "    for ans in answers:\n",
        "        ans_ = _normalize(ans)\n",
        "        prediction_ = _normalize(prediction)\n",
        "        if ans_ == prediction_:\n",
        "            em = 1\n",
        "            break\n",
        "    return em\n",
        "\n",
        "\n",
        "def evaluate_predictions(ref_ans, pred_ans):\n",
        "    F1, EM, TOTAL, SKIP = evaluate(ref_ans, pred_ans, verbose=False)\n",
        "    output_result = OrderedDict()\n",
        "    output_result['F1'] = '%.3f' % F1\n",
        "    output_result['EM'] = '%.3f' % EM\n",
        "    output_result['TOTAL'] = TOTAL\n",
        "    output_result['SKIP'] = SKIP\n",
        "    return output_result"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdmQShYyJCm7"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import transformers\n",
        "\n",
        "transformers.logging.set_verbosity_error()\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizerFast, TFBertModel, BertConfig\n",
        "\n",
        "\n",
        "def paragraph_selection(context, answer_text, answer_start, max_len=450):\n",
        "    \"\"\"\n",
        "    以答案为基本中心裁剪context, 并重新计算answer_start\n",
        "    :param max_len: BERT输入的最大长度\n",
        "    :param context: 段落文本\n",
        "    :param answer_text: 答案文本\n",
        "    :param answer_start: 答案开始位置\n",
        "    :return: context, answer_start\n",
        "    \"\"\"\n",
        "    standard = max_len - 30\n",
        "    standard_mid = standard // 2\n",
        "    if len(context) < standard:\n",
        "        return context, answer_start\n",
        "    answer_end = answer_start + len(answer_text)\n",
        "    if answer_end < standard:\n",
        "        return context, answer_start\n",
        "    answer_mid = (answer_start + answer_end) // 2\n",
        "    select_start = answer_mid - standard_mid\n",
        "    select_end = answer_mid + standard_mid\n",
        "    if select_start < 0:\n",
        "        select_start = 0\n",
        "    if select_end > len(context):\n",
        "        select_end = len(context)\n",
        "    context = context[select_start: select_end]\n",
        "    answer_start = context.find(answer_text)\n",
        "    if answer_start < 0:\n",
        "        print(select_start, select_end, len(context))\n",
        "    return context, answer_start\n",
        "\n",
        "\n",
        "def load_dataset(data_path):\n",
        "    with open(data_path) as f:\n",
        "        input_data = json.load(f)['data']\n",
        "\n",
        "    examples = []\n",
        "    for entry in tqdm(input_data):\n",
        "        for paragraph in entry[\"paragraphs\"]:\n",
        "            context = paragraph[\"context\"].strip()\n",
        "            try:\n",
        "                title = paragraph[\"title\"].strip()\n",
        "            except KeyError:\n",
        "                title = ''\n",
        "\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                qas_id = qa[\"id\"]\n",
        "                question = qa[\"question\"].strip()\n",
        "\n",
        "                is_impossible = False\n",
        "\n",
        "                if \"is_impossible\" in qa.keys():\n",
        "                    is_impossible = qa[\"is_impossible\"]\n",
        "\n",
        "                answer_starts = [answer[\"answer_start\"] for answer in qa.get(\"answers\", [])]\n",
        "                answers = [answer[\"text\"].strip() for answer in qa.get(\"answers\", [])]\n",
        "\n",
        "                if len(answer_starts) == 0 or answer_starts[0] == -1:\n",
        "                    examples.append({\n",
        "                        \"id\": qas_id,\n",
        "                        \"title\": title,\n",
        "                        \"context\": context,\n",
        "                        \"question\": question,\n",
        "                        \"answers\": answers,\n",
        "                        \"answer_starts\": answer_starts,\n",
        "                        \"is_impossible\": is_impossible\n",
        "                    })\n",
        "                else:\n",
        "                    answer_start = answer_starts[0]\n",
        "                    answer_text = answers[0]\n",
        "                    cut_context, answer_start = paragraph_selection(context, answer_text, answer_start, max_len=450)\n",
        "                    examples.append({\n",
        "                        \"id\": qas_id,\n",
        "                        \"title\": title,\n",
        "                        \"context\": cut_context,\n",
        "                        \"question\": question,\n",
        "                        \"answers\": [answer_text],\n",
        "                        \"answer_starts\": [answer_start],\n",
        "                        \"is_impossible\": is_impossible\n",
        "                    })\n",
        "    return examples\n",
        "\n",
        "\n",
        "def convert_to_features(examples, tokenizer, max_len=450, stride=128):\n",
        "    questions = [examples[i]['question'] for i in range(len(examples))]\n",
        "    contexts = [examples[i]['context'] for i in range(len(examples))]\n",
        "    tokenized_examples = tokenizer(questions,\n",
        "                                   contexts,\n",
        "                                   padding=\"max_length\",\n",
        "                                   max_length=max_len,\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=stride,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   return_overflowing_tokens=False)\n",
        "\n",
        "    tokenized_examples = pd.DataFrame.from_dict(tokenized_examples, orient=\"index\").T\n",
        "    tokenized_examples = tokenized_examples.to_dict(orient=\"records\")\n",
        "\n",
        "    for i, tokenized_example in enumerate(tqdm(tokenized_examples)):\n",
        "        input_ids = tokenized_example[\"input_ids\"]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        offsets = tokenized_example['offset_mapping']\n",
        "        sequence_ids = tokenized_example['token_type_ids']\n",
        "\n",
        "        answers = examples[i]['answers']\n",
        "        answer_starts = examples[i]['answer_starts']\n",
        "\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answer_starts) == 0 or answer_starts[0] == -1:\n",
        "            tokenized_examples[i][\"start_positions\"] = cls_index\n",
        "            tokenized_examples[i][\"end_positions\"] = cls_index\n",
        "            tokenized_examples[i]['answerable_label'] = 0\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answer_starts[0]\n",
        "            end_char = start_char + len(answers[0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 2\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "            token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and\n",
        "                    offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[i][\"start_positions\"] = cls_index\n",
        "                tokenized_examples[i][\"end_positions\"] = cls_index\n",
        "                tokenized_examples[i]['answerable_label'] = 0\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[i][\"start_positions\"] = token_start_index - 1\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[i][\"end_positions\"] = token_end_index + 1\n",
        "                tokenized_examples[i]['answerable_label'] = 1\n",
        "\n",
        "        tokenized_examples[i][\"example_id\"] = examples[i]['id']\n",
        "\n",
        "    dataset_dict = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"start_positions\": [],\n",
        "                    \"end_positions\": [], }\n",
        "    for item in tokenized_examples:\n",
        "        for key in dataset_dict:\n",
        "            dataset_dict[key].append(item[key])\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = np.array(dataset_dict[key])\n",
        "\n",
        "    x = [dataset_dict[\"input_ids\"], dataset_dict[\"token_type_ids\"], dataset_dict[\"attention_mask\"]]\n",
        "    y = [dataset_dict[\"start_positions\"], dataset_dict[\"end_positions\"]]\n",
        "\n",
        "    return tokenized_examples, x, y\n",
        "\n",
        "\n",
        "def build_model(model_path, max_len):\n",
        "    encoder = TFBertModel.from_pretrained(model_path, from_pt=True)\n",
        "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
        "    sequence_output= encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)[0]\n",
        "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(sequence_output)\n",
        "    start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(sequence_output)\n",
        "    end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "    start_probs = layers.Activation(keras.activations.softmax, name=\"start\")(start_logits)\n",
        "    end_probs = layers.Activation(keras.activations.softmax, name=\"end\")(end_logits)\n",
        "\n",
        "    bert_model = keras.Model(inputs=[input_ids, token_type_ids, attention_mask], outputs=[start_probs, end_probs],\n",
        "                             name=\"BERTForQuestionAnswer\")\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = keras.optimizers.Nadam(lr=3e-5)\n",
        "    bert_model.compile(optimizer=optimizer, loss=[loss, loss], metrics=['acc'])\n",
        "    return bert_model\n",
        "\n",
        "\n",
        "def TrainEMF1(eval_examples, tokenized_examples, tokenizer):\n",
        "    em, f1, total_count = 0, 0, 0\n",
        "    count = 0\n",
        "    for idx, example in enumerate(tqdm(eval_examples, desc=\"Evaluation\")):\n",
        "        total_count += 1\n",
        "        offsets = tokenized_examples[idx]['offset_mapping']\n",
        "        start = tokenized_examples[idx][\"start_positions\"]\n",
        "        end = tokenized_examples[idx][\"end_positions\"]\n",
        "        if (start >= len(offsets) or end >= len(offsets) or offsets[start] is None or\n",
        "                offsets[end] is None or offsets[start] == (0, 0) or offsets[end] == (0, 0)):\n",
        "            prediction = \"\"\n",
        "            print(start, end, offsets[start], offsets[end])\n",
        "            print(\"\".join(tokenizer.decode(tokenized_examples[idx][\"input_ids\"])))\n",
        "            print(example[\"answers\"])\n",
        "            count += 1\n",
        "        else:\n",
        "            pred_char_start = offsets[start][0]\n",
        "            pred_char_end = offsets[end][1]\n",
        "            prediction = example[\"context\"][pred_char_start:pred_char_end]\n",
        "\n",
        "        answers = example[\"answers\"]\n",
        "        f1 += calc_f1_score(answers, prediction)\n",
        "        em += calc_em_score(answers, prediction)\n",
        "    print(count)\n",
        "    f1_score = 100.0 * f1 / (total_count - 0)\n",
        "    em_score = 100.0 * em / (total_count - 0)\n",
        "    tqdm.write(f\"F1 score={f1_score:.3f},  EM score={em_score:.3f}\")\n",
        "\n",
        "\n",
        "class EM_F1Score(keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, eval_x, eval_y, eval_examples, tokenized_examples):\n",
        "        super().__init__()\n",
        "        self.x_eval = eval_x\n",
        "        self.y_eval = eval_y\n",
        "        self.eval_examples = eval_examples\n",
        "        self.tokenized_examples = tokenized_examples\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pred_start, pred_end = self.model.predict(self.x_eval)\n",
        "        em, f1, total_count = 0, 0, 0\n",
        "        for idx, (start, end) in enumerate(tqdm(list(zip(pred_start, pred_end)), desc=\"Evaluation\")):\n",
        "            total_count += 1\n",
        "            example = self.eval_examples[idx]\n",
        "            offsets = self.tokenized_examples[idx]['offset_mapping']\n",
        "            start = np.argmax(start)\n",
        "            end = np.argmax(end)\n",
        "            if (start >= len(offsets) or end >= len(offsets) or offsets[start] is None or\n",
        "                    offsets[end] is None or offsets[start] == (0, 0) or offsets[end] == (0, 0)):\n",
        "                prediction = \"\"\n",
        "            else:\n",
        "                pred_char_start = offsets[start][0]\n",
        "                pred_char_end = offsets[end][1]\n",
        "                prediction = example[\"context\"][pred_char_start:pred_char_end]\n",
        "\n",
        "            answers = example[\"answers\"]\n",
        "            f1 += calc_f1_score(answers, prediction)\n",
        "            em += calc_em_score(answers, prediction)\n",
        "        f1_score = 100.0 * f1 / total_count\n",
        "        em_score = 100.0 * em / total_count\n",
        "        logs['F1'] = f1_score\n",
        "        logs['EM'] = em_score\n",
        "        tqdm.write(f\"epoch={epoch + 1}, F1 score={f1_score:.3f},  EM score={em_score:.3f}\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRNDMiAnJJ_s",
        "outputId": "70e17ed5-f3ee-4eb6-d172-4db62d538c2d"
      },
      "source": [
        "train_path = './cmrc2018/cmrc2018_train.json'\n",
        "eval_path = './cmrc2018/cmrc2018_dev.json'\n",
        "\n",
        "max_len = 450\n",
        "stride = 128\n",
        "bert_model_path = \"./chinese_roberta_wwm_ext_pytorch\"\n",
        "tokenizer = BertTokenizerFast.from_pretrained(bert_model_path)\n",
        "train_examples = load_dataset(data_path=train_path)\n",
        "train_tokenized_examples, x_train, y_train = convert_to_features(train_examples, tokenizer, max_len=max_len,\n",
        "                                                                  stride=stride)\n",
        "print(f\"{len(train_examples)} train_examples {len(train_tokenized_examples)} train_tokenized_examples.\")\n",
        "\n",
        "eval_examples = load_dataset(data_path=eval_path)\n",
        "eval_tokenized_examples, x_eval, y_eval = convert_to_features(eval_examples, tokenizer, max_len=max_len,\n",
        "                                                              stride=stride)\n",
        "print(f\"{len(eval_examples)} eval_examples {len(eval_tokenized_examples)} eval_tokenized_examples.\")\n",
        "\n",
        "# TrainEMF1(train_examples, train_tokenized_examples, tokenizer)\n",
        "# TrainEMF1(eval_examples, eval_tokenized_examples, tokenizer)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2403/2403 [00:00<00:00, 64738.72it/s]\n",
            "100%|██████████| 10142/10142 [00:00<00:00, 21351.38it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10142 train_examples 10142 train_tokenized_examples.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 848/848 [00:00<00:00, 53434.64it/s]\n",
            "100%|██████████| 3219/3219 [00:00<00:00, 20501.96it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3219 eval_examples 3219 eval_tokenized_examples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh4HwHfKJUSe",
        "outputId": "07ef54a7-8ed2-402c-9508-3fb40c8952ce"
      },
      "source": [
        "model = build_model(model_path=bert_model_path, max_len=max_len)\n",
        "EMF1_Callback = EM_F1Score(x_eval, y_eval, eval_examples, eval_tokenized_examples)\n",
        "model.fit(x_train, y_train, epochs=3, verbose=1, batch_size=10, callbacks=[EMF1_Callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            " 136/1015 [===>..........................] - ETA: 9:00 - loss: 8.2568 - start_loss: 4.1551 - end_loss: 4.1017 - start_acc: 0.1697 - end_acc: 0.1330"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3IuqO4uK04M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}